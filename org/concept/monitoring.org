#+title: Monitoring / Observability

* Meta
- https://sre.google/sre-book/monitoring-distributed-systems/
- https://logz.io/blog/top-10-mistakes-building-observability-dashboards/
- https://blog.danslimmon.com/2017/10/02/what-makes-a-good-alert/

* Black Box Versus White Box
- summary from https://sre.google/sre-book/monitoring-distributed-systems/#black-box-versus-white-box-q8sJuw
- heavy use of white-box monitoring with modest but critical uses of black-box monitoring.
  - black-box monitoring is symptom-oriented and represents active—not predicted—problems: "The system isn’t working correctly, right now."
  - white-box monitoring depends on the ability to inspect the innards of the system, such as logs or HTTP endpoints, with instrumentation
    - therefore allows detection of imminent problems, failures masked by retries, and so forth
- one person’s symptom is another person’s cause. For example, suppose that a database’s performance is slow
  - slow database reads are a symptom for the database SRE who detects them
    - However, for the frontend SRE observing a slow website, the same slow database reads are a cause
  - therefore, white-box monitoring is sometimes symptom-oriented, and sometimes cause-oriented, depending on just how informative your white-box is
- When collecting telemetry for debugging,
  - white-box monitoring is essential
    - if web servers seem slow on database-heavy requests, you need to know both how fast the web server perceives the database to be, and how fast the database believes itself to be
      - otherwise, you can’t distinguish an actually slow database server from a network problem between your web server and your database
- for paging,
  - black-box monitoring has the key benefit of forcing discipline to only nag a human when a problem is both already ongoing and contributing to real symptoms
  - on the other hand, for not-yet-occurring but imminent problems, black-box monitoring is fairly useless

* Metrics
** The RED Method: key metrics for microservices architecture
/fits well for request based services/
https://www.weave.works/blog/the-red-method-key-metrics-for-microservices-architecture/
- (Request) Rate - the number of requests, per second, you services are serving.
- (Request) Errors - the number of failed requests per second.
- (Request) Duration - distributions of the amount of time each request takes.
** The Use Method
/resource-oriented perspective/
- https://www.brendangregg.com/usemethod.html
  - Utilization: the average time that the resource was busy servicing work
    - /as a percent over a time interval. eg, "one disk is running at 90% utilization"./
  - Saturation: the degree to which the resource has extra work which it can't service, often queued
    - /as a queue length. eg, "the CPUs have an average run queue length of four"./
  - Errors: the count of error events
    - /scalar counts. eg, "this network interface has had fifty late collisions"./
- https://www.circonus.com/2017/08/system-monitoring-with-the-use-dashboard/
** The Four Golden Signals
- summary from https://sre.google/sre-book/monitoring-distributed-systems/#xref_monitoring_golden-signals
*** Latency
- the time it takes to service a request
- it’s important to distinguish between the latency of successful requests and the latency of failed requests
  - For example,
    - an HTTP 500 error triggered due to loss of connection to a database or other critical backend might be served very quickly
    - however, as an HTTP 500 error indicates a failed request
    - factoring 500s into your overall latency might result in misleading calculations
    - On the other hand, a slow error is even worse than a fast error!
  - therefore, it’s important to track error latency, as opposed to just filtering out errors
*** Traffic
- a measure of how much demand is being placed on your system, measured in a high-level system-specific metric
  - for a web service, this measurement is usually HTTP requests per second, perhaps broken out by the nature of the requests (e.g., static versus dynamic content)
  - for  an audio streaming system, this measurement might focus on network I/O rate or concurrent sessions
  - for a key-value storage system, this measurement might be transactions and retrievals per second
*** Errors
- the rate of requests that fail
  - either explicitly
    - e.g., HTTP 500s
  - implicitly
    - for example, an HTTP 200 success response, but coupled with the wrong content
  - or by policy
    - for example, "If you committed to one-second response times, any request over one second is an error"
- where protocol response codes are insufficient to express all failure conditions, secondary (internal) protocols may be necessary to track partial failure modes
  - monitoring these cases can be drastically different:
    - catching HTTP 500s at your load balancer can do a decent job of catching all completely failed requests
    - while only end-to-end system tests can detect that you’re serving the wrong content
*** Saturation
- How "full" your service is.
  - A measure of your system fraction,
    - emphasizing the resources that are most constrained
      - e.g.,
        - in a memory-constrained system, show memory;
        - in an I/O-constrained system, show I/O
  - note that many systems degrade in performance before they achieve 100% utilization
  - so having a utilization target is essential
- supplement with other measurements
  - in complex systems, saturation can be supplemented with higher-level load measurement:
    - can your service properly
      - handle double the traffic?
      - handle only 10% more traffic?
      - or handle even less traffic than it currently receives?
  - for very simple services that have no parameters that alter the complexity of the request
    - (e.g., "Give me a nonce" or "I need a globally unique monotonic integer")
    - that rarely change configuration, a static value from a load test might be adequate
- most services need to use indirect signals like CPU utilization or network bandwidth that have a known upper bound
- latency increases are often a leading indicator of saturation
- measuring your 99th percentile response time over some small window (e.g., one minute) can give a very early signal of saturation
- finally, saturation is also concerned with predictions of impending saturation
  - such as "It looks like your database will fill its hard drive in 4 hours."

*** (Wrapping Up)
If you measure all four golden signals and page a human when one signal is problematic (or, in the case of saturation, nearly problematic), your service will be at least decently covered by monitoring.

** P99
- https://stackoverflow.com/questions/12808934/what-is-p99-latency
- https://www.quora.com/What-is-p99-latency
- https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Statistics-definitions.html#Percentile-versus-Trimmed-Mean
- https://medium.com/last9/your-percentiles-are-incorrect-p99-of-the-times-11436c97d524
- https://igor.io/latency/
- https://www.gomomento.com/blog/why-tail-latencies-matter

* OpenMetrics
https://openmetrics.io/
Basically Prometheus format evolved into open metrics format so this format can be used more widely at different tools

* OpenTelemetry
https://opentelemetry.io/
#+begin_quote
OpenTelemetry is a collection of tools, APIs, and SDKs. Use it to instrument, generate, collect, and export telemetry data (metrics, logs, and traces) to help you analyze your software’s performance and behavior.

OpenTelemetry is generally available across  [[https://opentelemetry.io/docs/instrumentation/][several languages]] and is suitable for use.
#+end_quote

https://github.blog/2021-05-26-why-and-how-github-is-adopting-opentelemetry/
https://docs.datadoghq.com/tracing/trace_collection/open_standards/

** Relationship with Vector
https://vector.dev/docs/setup/going-to-prod/architecting/

* OpenMetrics and OpenTelemetry together
They both are CNCF projects
- https://signoz.io/blog/openmetrics-vs-opentelemetry/
- https://opentelemetry.io/docs/reference/specification/compatibility/openmetrics/

So a meaning of OpenTelemetry and OpenMetrics could be like using a OpenTelemetry client (i.e. [[https://opentelemetry.io/docs/instrumentation/js/getting-started/nodejs/][NodeJS]]) to export metrics (i.e. instead of =prom-client=) and the OpenMetrics format could be ingested into different backend (i.e. Datadog)

* ETC
- https://cloud.google.com/blog/products/devops-sre/welcome-to-the-museum-of-modern-borgmon-art
- [[https://gist.github.com/ryuheechul/c6643e797d14f83bed2eb2f7614230e0][summary of monitoring distributed systems in Korean]]
